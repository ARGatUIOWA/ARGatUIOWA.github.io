<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://argatuiowa.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://argatuiowa.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-18T15:31:13+00:00</updated><id>https://argatuiowa.github.io/feed.xml</id><title type="html">Algorithms Reading Group</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Hongyan Ji presents “Efficient Algorithms for Constructing Very Sparse Spanners and Emulators”</title><link href="https://argatuiowa.github.io/blog/2025/Hongyan/" rel="alternate" type="text/html" title="Hongyan Ji presents “Efficient Algorithms for Constructing Very Sparse Spanners and Emulators”"/><published>2025-09-18T06:00:00+00:00</published><updated>2025-09-18T06:00:00+00:00</updated><id>https://argatuiowa.github.io/blog/2025/Hongyan</id><content type="html" xml:base="https://argatuiowa.github.io/blog/2025/Hongyan/"><![CDATA[<p><strong>Abstract</strong>: Miller et al. \cite{MPVX15} devised a distributed\footnote{They actually showed a PRAM algorithm. The distributed algorithm with these properties is implicit in \cite{MPVX15}.} algorithm in the CONGEST model, that given a parameter \(k = 1, 2, \dots\), constructs an \(O(k)\)-spanner of an input unweighted \(n\)-vertex graph with \(O(n^{1+1/k})\) expected edges in \(O(k)\) rounds of communication. In this paper we improve the result of \cite{MPVX15}, by showing a \(k\)-round distributed algorithm in the same model, that constructs a \((2k-1)\)-spanner with \(O(n^{1+1/k}/\epsilon)\) edges, with probability \(1 - \epsilon\), for any \(\epsilon &gt; 0\). Moreover, when \(k = \omega(\log n)\), our algorithm produces (still in \(k\) rounds) <em>ultra-sparse</em> spanners, i.e., spanners of size \(n(1+o(1))\), with probability \(1 - o(1)\). To our knowledge, this is the first distributed algorithm in the CONGEST or in the PRAM models that constructs spanners or skeletons (i.e., connected spanning subgraphs) that sparse. Our algorithm can also be implemented in linear time in the standard centralized model, and for large \(k\), it provides spanners that are sparser than any other spanner given by a known (near-)linear time algorithm.</p> <p>We also devise improved bounds (and algorithms realizing these bounds) for \((1+\epsilon, \beta)\)-spanners and emulators. In particular, we show that for any unweighted \(n\)-vertex graph and any \(\epsilon &gt; 0\), there exists a \((1+\epsilon, (\frac{\log\log n}{\epsilon \log n})^{\log\log n})\)-emulator with \(O(n)\) edges. All previous constructions of \((1+\epsilon, \beta)\)-spanners and emulators employ a superlinear number of edges, for all choices of parameters.</p> <p>Finally, we provide some applications of our results to approximate shortest paths’ computation in unweighted graphs.</p> <p><a href="https://arxiv.org/abs/1607.08337">Efficient Algorithms for Constructing Very Sparse Spanners and Emulators</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Abstract: Miller et al. \cite{MPVX15} devised a distributed\footnote{They actually showed a PRAM algorithm. The distributed algorithm with these properties is implicit in \cite{MPVX15}.} algorithm in the CONGEST model, that given a parameter \(k = 1, 2, \dots\), constructs an \(O(k)\)-spanner of an input unweighted \(n\)-vertex graph with \(O(n^{1+1/k})\) expected edges in \(O(k)\) rounds of communication. In this paper we improve the result of \cite{MPVX15}, by showing a \(k\)-round distributed algorithm in the same model, that constructs a \((2k-1)\)-spanner with \(O(n^{1+1/k}/\epsilon)\) edges, with probability \(1 - \epsilon\), for any \(\epsilon &gt; 0\). Moreover, when \(k = \omega(\log n)\), our algorithm produces (still in \(k\) rounds) ultra-sparse spanners, i.e., spanners of size \(n(1+o(1))\), with probability \(1 - o(1)\). To our knowledge, this is the first distributed algorithm in the CONGEST or in the PRAM models that constructs spanners or skeletons (i.e., connected spanning subgraphs) that sparse. Our algorithm can also be implemented in linear time in the standard centralized model, and for large \(k\), it provides spanners that are sparser than any other spanner given by a known (near-)linear time algorithm.]]></summary></entry><entry><title type="html">Joshua Sobel presents “Work-Efficient Parallel Counting via Sampling”</title><link href="https://argatuiowa.github.io/blog/2025/Josh/" rel="alternate" type="text/html" title="Joshua Sobel presents “Work-Efficient Parallel Counting via Sampling”"/><published>2025-09-04T06:00:00+00:00</published><updated>2025-09-04T06:00:00+00:00</updated><id>https://argatuiowa.github.io/blog/2025/Josh</id><content type="html" xml:base="https://argatuiowa.github.io/blog/2025/Josh/"><![CDATA[<p><strong>Abstract</strong>: A canonical approach to approximating the partition function of a Gibbs distribution via sampling is simulated annealing. This method has led to efficient reductions from counting to sampling, including: \(\bullet\) classic non-adaptive (parallel) algorithms with sub-optimal cost (Dyer-Frieze-Kannan ‘89; Bezáková-Štefankovič-Vazirani-Vigoda ‘08); \(\bullet\) adaptive (sequential) algorithms with near-optimal cost (Štefankovič-Vempala-Vigoda ‘09; Huber ‘15; Kolmogorov ‘18; Harris-Kolmogorov ‘24). We present an algorithm that achieves both near-optimal total work and efficient parallelism, providing a reduction from counting to sampling with logarithmic depth and near-optimal work. As consequences, we obtain work-efficient parallel counting algorithms for several important models, including the hardcore and Ising models within the uniqueness regime.</p> <p><a href="https://arxiv.org/abs/2408.09719">Work-Efficient Parallel Counting via Sampling</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Abstract: A canonical approach to approximating the partition function of a Gibbs distribution via sampling is simulated annealing. This method has led to efficient reductions from counting to sampling, including: \(\bullet\) classic non-adaptive (parallel) algorithms with sub-optimal cost (Dyer-Frieze-Kannan ‘89; Bezáková-Štefankovič-Vazirani-Vigoda ‘08); \(\bullet\) adaptive (sequential) algorithms with near-optimal cost (Štefankovič-Vempala-Vigoda ‘09; Huber ‘15; Kolmogorov ‘18; Harris-Kolmogorov ‘24). We present an algorithm that achieves both near-optimal total work and efficient parallelism, providing a reduction from counting to sampling with logarithmic depth and near-optimal work. As consequences, we obtain work-efficient parallel counting algorithms for several important models, including the hardcore and Ising models within the uniqueness regime.]]></summary></entry></feed>